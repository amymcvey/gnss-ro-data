# GNSS RO in AWS Utilities

**AWS Location**: s3://gnss-ro-data

**AWS Region**: us-east-1  

**Managing Organization**: Atmospheric and Environmental Research, Inc.

*Correspondence:* Stephen Leroy (sleroy@aer.com) or Amy McVey (amcvey@aer.com)

============================================


## Introduction

There are two options to performing database inquiries on the radio occultation 
data contained in the AWS Registry of Open Data. The first is to use a utility 
provided in the Python module **awsgnssroutils.py**, which enables a Python 
programmer to query the online RO database, mirror the contents of the database 
on a local file system as needed, and download RO data files. The second is to 
construct a DynamoDB database based on the contents of the AWS Registry of Open 
Data for GNSS RO. DynamoDB is a much more efficient database engine than the 
Python module **awsgnssroutils.py**, but constructing the DynamoDB table and 
executing the queries are more complicated. In the material below, we introduce 
both the Python module and offer instruction on buildling a DynamoDB table and 
querying it. 

## Python database module

*Prerequisites:* numpy, boto3, s3fs

The Python module is **awsgnssroutils.py**, which can be downloaded from this 
repository. It contains the definitions of two object classes: *RODatabaseClient* 
and *OccList*. *RODatabaseClient* functions as a portal to a database of the 
metadata associated with all of the Earth GNSS RO data in the AWS Registry of 
Open Data. It contains two methods that generate the metadata for a subsets 
of RO data, one being the restoration of a previously saved subset, and the 
second being the generation of a new subset according to RO mission(s) and a 
range of date-times for RO data. The subsets come in the form of instances of 
*OccList*, which offers methods to filter/subset the list to create another 
one, download the RO data files, and generate the values of RO metadata such as 
geolocation and local time.  Inline documentation in the classes 
*RODatabaseClass* and *OccList* is complete. 

<span style="color:red"> 
The jupyter notebook <b>demonstration_notebook.ipynb</b> demonstrates how to use 
the <i>RODatabaseClient</i> and <i>OccList</i> classes to query the RO database. 
</span>

### RODatabaseClient

The *RODatabaseClient* class creates a portal to the database (of metadata) on 
RO in the AWS Registry of Open Data. Instantiation offers three keywords: 
"repository", "version", and "update". By default, the client establishes a 
connection to the metadata on RO contained in AWS Open Data and reads directly 
from the AWS S3 bucket s3://gnss-ro-data. This is an inefficient way to go 
about querying the database, which is rather large, so an option is provided 
to create a local repository of the RO database on the local file system. The 
local repository is not a complete mirror, however; it only stores the contents 
of the RO database locally that have been requested in prior queries. If the 
"repository" key is set to a local path, the contents of previous queries are 
stored in that path. Second, generally there are several versions of the 
AWS RO data, and this key allows the user to specify which version to access. 
(As of 23 Nov 2022, the most up-to-date version is "v1.1", the default.) 
Third, the user has the option to update the repository on the local file 
system if the latter are older than what is hosted in the AWS S3 bucket. Only 
those metadata files that exist in the local repository are updates; no new 
metadata files are downloaded. 

**We highly recommend that users create a repository of the metadata on their 
local file systems.**

```
from awsgnssroutils import RODatabaseClient
db = RODatabaseClient( repository="rodatabase" )
```
will create a gateway "db" to the RO database in the Open Data Registry and 
copy all that is retrieved by subsequent queries of the AWS database into 
the local directory "rodatabase". 

The instance of *RODatabaseClient* can then be used to create instances of 
*OccList* by either querying the database or by restoring a previously 
saved *OccList*. Any query must constrain a search by either a time range 
or by RO mission(s) or both. For example, one get query the database for all 
RO data over the year 2009 by 
```
occs2009 = db.query( datetimerange=("2009-01-01","2010-01-01") )
```
in which both elements of the 2-tuple for "datetimerange" are ISO-format 
datetime strings. A user can also query the database by mission(s) by 
```
occs = db.query( missions=("sacc","tsx","tdx") )
```
which will get metadata for all RO soundings by the RO missions SAC-C, 
TerraSAR-X, and TanDEM-X. Both "occs2009" and "occs" are instances of 
*OccList*. Use of the *RODatabaseClient.restore* method is illustrated below, 
in the description of the *OccList.save* method. 

### OccList

Once an instance of *OccList* is generated by *RODatabaseClient.query* or by 
*RODatabaseClient.restore*, many possible manipulation of that instance are 
made possible by the methods it contains. The methods allow for further 
filtering/subsetting of the instance, getting information on the instance, 
retrieving geolocation metadata on the RO soundings in the instance, downloading 
a desired file type (*calibratedPhase*, *refractivityRetrieval*, 
*atmosphericRetrieval*) of RO data from the AWS Registry of Open Data, and 
saving the instance to a JSON-format file for future restoration by the 
*RODatabaseClient.restore* method. 

Using "occs" as obtained above, that instance of RO metadata can be filtered in 
time, longitude, latitude, and local time by 
```
occs_2011_US_midnight = occs.filter( datetimerange=("2011-01-01","2012-01-01"), 
	longituderange=(-110,-70), latituderange=(25,55), localtimerange=(22,2) )
```
which filters "occs" to just the year 2011, over the contiguous U.S. by 
restricting longitude and latitude, and to just those soundings within two hours 
of local midnight. (The units of localtimerange values are hours, and the 
range can wrap around midnight, like longituderange can wrap around the date 
line.) It is also possible to filter by *transmitters* (GNSS transmitting 
satellites), by *GNSSconstellation* (e.g., "G" for GPS, "R" for GLONASS, etc.), 
and by sounding *geometry* ("rising" or "setting"). 

It is also possible to get metadata values from an *OccList* instance using the 
*OccList.info* method. For example one can retrieve a list of RO missions in the 
instance, a list of tracked GNSS transmitters, ranges of longitudes, latitudes, 
local times, date-times, etc. 
```
missions_in_occs2009 = occs2009.info( "mission" )
geometry_in_occs2009 = occs2009.info( "geometry" )
longituderange_in_occs2009 = occs2009.info( "longitude" )
```
The "missions_in_occs2009" is a list of missions in "occs2009"; "geometry_in_occs2009" 
is a dictionary with keywords "nrising" and "nsetting" that count the numbers of 
rising and setting RO soundings in "occs2009"; and "longituderange_in_occs2009" is a 
dictionary with keywords "min" and "max" that indicate the minimum and maximum 
longitude of the RO soundings in "occs2009". 

It is possible to get metadata values on geolocation from an *OccList* instance 
as well. For example, 
```
longitudes = occs2009.values( "longitude" )
latitudes = occs2009.values( "latitude" )
```
yields ndarrays of longitude and latitude for all RO soundings ever obtained in 
the year 2009.

Existing instances of *OccList* can be saved for future restoration using the 
*OccList.save* and the *RODatabaseClient.restore* methods. For example, 
```
occs_2011_US_midnight.save( "savefile.json" )
occs_restored = db.restore( "savefile.json" )
```
saves "occs_2011_US_midnight" to the JSON file "savefile.json" and restores it 
to the new *OccList* instance "occs_restored". 

Finally, it is possible to download RO data using the *OccList.download* method. 
In order to do so for 13 February 2009, for example, try 
```
occ_day = occs2009.filter( datetimerange=("2009-02-13","2009-02-14") )
occfiles = odd_day.download( "ucar_atmosphericRetrieval", "rodata", keep_aws_structure=True )
```
which downloads all *atmosphericRetrieval* files contributed by the UCAR 
RO processing center for one day, and does so in a way that maintains the same directory 
structure as in the AWS Registry of Open Data S3 bucket with the local directory 
"rodata" as the root of the download. 


## AWS DynamoDB 

*Prerequisites:* 
In order for any of this to work, you must first get an account
with AWS and obtain up-to-date authentication tokens. Typically, these authentication
tokens will be referenced by a profile name, and that profile name should be set
in the configuration section of the header of the import_gnss-ro_dynamoDB.py script.
The variable that should be set is "aws_profile".

### Python environment

The specific packages required for each utility is listed in-line for each respective file. To
easily create an environment to run the included utilities please see the
[Install_python_miniconda_linux.sh](http://github.com/gnss-ro/aws-opendata/blob/master/utilities/Install_python_miniconda_linux.sh)
bash script. This will install the latest python version via Miniconda and the necessary
python packages.

### Implement DynamoDB table

A utility is provided to assist you in creating your own DynamoDB database
table given the JSON files hosted in *s3://gnss-ro-data/dynamo_export_subsets*.
There is a faster more efficient way however complex, using AWS DataPipeline.  We have
also created a python script that uses boto3 to loop through and create the database one item at a time.
However this second way is very time consuming to recreate the entire table, but great for partial chunks.

#### AWS DataPipeline:
This option will import the full table to your AWS DynamoDB within a few hours.
The instructions to set the up are included here in "DynamoDB_full_import_instructions.txt". These instructions
assume you have full access to your AWS account.

To update it you can use the Python script below and simply type in the most recent month or day.
For example:

```
python3 import_gnss-ro_dynamoDB.py --dynamodb_table_name my_ro_database --date_str "202208"
```

#### Python and DynamoDB
The script is **import_gnss-ro_dynamoDB.py**. It creates a DynamoDB database
given an RO mission name and optionally a date range for RO soundings. For example,
in order to import all CHAMP RO data into your own DynamoDB table, execute the command

```
python3 import_gnss-ro_dynamoDB.py --dynamodb_table_name my_ro_database --mission champ
```

In order to import just one year of CHAMP entries into your database, try

```
python3 import_gnss-ro_dynamoDB.py --dynamodb_table_name my_ro_database --mission champ --date_str "2003"
```

for all of year 2003. Note that you can define the table name of your own database in place of
"my_ro_database". To import just one month of CHAMP entries in the database, try

```
python3 import_gnss-ro_dynamoDB.py --dynamodb_table_name my_ro_database --mission champ --date_str "2003-02"
python3 import_gnss-ro_dynamoDB.py --dynamodb_table_name my_ro_database --mission champ --date_str "2003-02-14"
```

If you wish to import all entries into your database, ...

```
python3 import_gnss-ro_dynamoDB.py --full
```

but this operation can take a very long time, up to several days.


### Database design and usage

A DynamoDB database is premised on the usage of "partition" and "sort" keys. Together, they uniquely
define an RO sounding. In this case, the partition key is "leo-ttt" where "leo" is the low-Earth-orbiting
receiver name and "ttt" is the GNSS transmitter identifier. The sort key is "yyyy-mm-dd-hh-mm" (year,
month, day, hour, minute) of the RO sounding. See the main
[Readme document](http://github.com/gnss-ro/aws-opendata/blob/master/Readme.md). Any query of the database requires
a unique specification of the partition key and at least a partial definition of the sort key. Each
entry (for a unique radio occultation sounding) contains information on the time, longitude, latitude,
solar time (local time) of the occultation, whether it is a rising or setting occultation, and pointers to the
various data files in the S3 bucket.

For an explicit demonstration of how to use the DynamoDB database, see the
[tutorial demonstrations](http://github.com/gnss-ro/aws-opendata/tree/master/tutorials). Creating your 
own DynamoDB table of all RO data can be extraordinarily time consuming: using a single computer it can 
take several weeks! We instead using the AWS service DataPipeline. The instructions for creating your 
own DynamoDB table by DataPipeline are below. When implemented correctly, it should take only a few hours 
to create your own DynamoDB table of RO metadata. 

The prerequisites are as follows: 
* AWS console access
* DynamoDB table created as such:
    - Go to the dynamoDB Service in the AWS console
    - Select "Create table"
    - Table name = "gnss-ro-import"
    - Partition key = "leo-ttt"
    - Sort key = "date-time"
    - Select Custom Settings
    - Choose "Capacity mode" = On-demand
    - Finish by selecting "Create table"

Instructions:
1. In your AWS console, go to the Data Pipeline Service
2. Click "create new pipeline"
3. Enter a Name and Description of your choice
4. For Source: choose "DynamoDB Templates" >> "Import DynamoDB backup data from S3"
5. Input s3 folder: s3://gnss-ro-data/dynamo/v1.1/export_subsets/
6. Target DynamoDB table name: "gnss-ro-import"  (must match the above table name)
7. DynamoDB write throughput ratio: 0.9
8. Region of the DynamoDB table: "us-east-1"
9. under Schedule, select Run "on pipeline activation"
10.Disable logging
11. Select "Activate" at the bottom.  The IAM roles should be created for you assuming the IAM role you are signed in as has permissions.
12. To update this table you can run the "import_gnss-ro_dynamoDB.py" with the date string option.

Note: this pipeline may take a bit to start up, but should import all data into the specified DynamoDB table in 2-4 hours.
