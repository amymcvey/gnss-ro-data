# GNSS RO Reformat-Catalog System (RORefCat)

This code is comprised of three command line executables and six software packages to enable processing of GNSS radio occultation data from multiple missions and processing centers into unified standardized formats defined during the NASA ACCESS 2019 project "GNSS Radio Occultation Data in the AWS Cloud". A database is also created to facilite RO event querying, which is done through the awsgnssroutils code on GitHub and PyPi. The three executables are found in the "bin" directory: 

- Createjobs
- Batchprocess
- Liveupdate

The six software packages are 

- Database 
- Versions
- Reformatters
- Missions
- Utilities
- GNSSsatellites

The packages are each associated with a unique functionality. A few of them are designed with extensibility in mind: it is possible to augment this system to include new data streams from contributing processing centers, to include new RO missions, and define new standardized output formats. The system is intended to run in a Docker container; the Docker image can be built using the build.sh script. 

In what follows, we first describe the functionality of the executables, then the architecture of the RORefCat system, and finally how it can be augmented/extended. 

Dependencies: astropy(4.3.1), numpy, scipy, boto3, s3fs, netcdf4 

## Executables 

In short, **batchprocess** reformats level 1 and level 2 RO data as contributed by an independent processing center into one of the available AWS standardized formats. The list of input files to be reformatted by **batchprocess** are generated by the **createjobs** executable. Finally, **liveupdate** is used to download "new" RO data from a contributing processing center, unpack (untar) the data, and reformat the data by **createjobs**/**batchprocess**. Their usage follows. 

The system can be run in a "test" mode by setting the environment variable "TEST" to a non-null value. In this case, the output reformatted files are written into a "test" S3 bucket and the catalog written to a "test" DynamoDB table. 

### Createjobs

This executable creates the JSON files nessesary for batchprocessing. The JSON files contain lists of RO data files obtained from RO processing centers that should be reformatted and catalogued. A single call to **createjobs** will create an arbitrary number of JSON files. The number of incoming files that will be written into a single JSON file is between 1,000 and 3,000, optimized so as to make subsequent execution of **batchprocess** take a significant fraction of an hour. 

**Createjobs** inspects an S3 bucket where the incoming RO data have been downloaded, checks the available files against a master rules file to decide whether the data should be reformatted or not, and composes a JSON files that define lists of files to be reformatted by **batchprocess**. The master rules files dictate which dates, missions, and processing versions should be ingested into the AWS Registry of Open Data repository of RO data. 

##### Usage

The **createjobs** executable is run in a Docker container by 

```
docker run -i --rm --env TEST=1 --env-file envs.lis ro-processing-framework createjobs <processing_center> <mission> <level> --version <version> --liveupdate --verbose --daterange 2006-01-01:2006-12-31
```

AWS credentials to access the AWS account where the system is run are passed by way of the file "envs.lis". Generation of the file envs.lis depends upon how your AWS account credentials are created. The user should do what is necessary to manifest the security tokens in the local environment and then create the file by 
```
env | grep "^AWS_" >! envs.lis
```

The name of the Docker image being run is "ro-processing-framework", and the name of the program being run inside the Docker container is "createjobs". 

The files to be scanned are contributed by *processing_center*. It must be a member of the list Reformatters.valid_processing_centers. That list includes "ucar", "romsaf", "jpl", and "eumetsat" at the time of writing this Readme. 

The RO files to be scanned are taken from RO *mission*. It must be a member of the set Missions.valid_missions.keys(). 

The RO files to be scanned are for *level* of an RO processing system. The *level* must be "level1b" (excess phase data), "level2a" (bending angle and refractivity retrievals), or "level2b" (1DVar retrievals of pressure, temperature, and water vapor). 

The files should be reformatted into AWS file format *version*. It must be a member of the list Versions.valid_versions. 

The files to be scanned must reside in an S3 bucket corresponding to the live-update streams when the *--liveupdate* option is set. 

The files to be scanned must correspond to the date-range "date1:date2", an inclusive range where date1 and date2 have formats YYYY-MM-DD: YYYY for year, MM for zero-padded month, and DD for zero-padded day. 

Because the Docker image is constructed to use "bash" as the entrypoint, an example command to submit an AWS batch job that executes **createjobs** is 
```
["createjobs", "ucar", "cosmic1", "level1b", "--version", "1.1"]
```
Note that it is possible to reconstruct the entire repository of RO data in the AWS Registry of Open Data by looping through all processing centers, missions, and levels and subsequently submitting **batchprocess** jobs for each of the JSON files that are created by the loop. 

Dependencies: None

#### Batchprocess

This executable will reformat and catalogue the RO data files defined in a JSON file created by **createjobs**. Each file takes a few seconds to process. 

##### Usage

**Batchprocess** can be run in a Docker container using a command such as...
```
docker run -i --rm --env TEST=1 --env-file envs.lis ro-processing-framework batchprocess <s3URI json> --version <version> 
```

The TEST variable and envs.lis file are explained in the documentation above for **createjobs**. 

The program to be run in the Docker container is "batchprocess". 

The "s3URI json" is a uniform resource locator for the JSON file containing the incoming file list in an S3 bucket. An example is "s3://RORefCat-scripts/ucar/batchprocessing_*.json". 

The *version* must be a member of the list Versions.valid_versions. 

This command can be submitted as an AWS Batch job as...
```
["batchprocess", "s3://gnss-ro-data-test/batchprocess-jobs/test_champ_atmPhs_repro2016_2007_006.json", "--version", "1.1"]
```

Dependencies: A JSON file as generated by **createjobs**. 


### Liveupdate

This program ingests a tarball and processing center from which to download.  It untars and copies the RO files into an S3 bucket and then creates batchprocess*.json files to send to **batchprocess** for processing.  This program run modes for **liveupdate** including the following:
- "webscrape": download and untar files from contributing processing centers and copy the files into a local S3 bucket. This is relevant for processing centers "romsaf" and "ucar" only. 
- "sync": synchronize the staging bucket for the AWS Registry of Open Data to the Open Data public bucket provided a prefix into the staging bucket. 
- "export": Exports the DynamoDB metadata table to JSON files in "export_raw" format in the AWS Registry of Open Data bucket "dynamodb" directory. It also parses the export_raw JSON files for eventual conversion into the "export_subsets" JSON files described by the following "convert" functionality.  
- "convert": Converts the export_raw JSON files into the export_subsets JSON files in the Registry of Open Data dynamo hierarchy. It is these files that are used by the API **awsgnssroutils**.  

##### Usage

Execute **liveupdate** in a Docker container as...

```
docker run -i --rm --env TEST=1 --env-file envs.lis ro-processing-framework liveupdate_wrapper webscrape 1.1 --tarfile <tarball path to download> 
```
The *liveupdate_wrapper* is a script that wraps the liveupdate functions described above. In this example, the tarball points to a remote tarball of RO data from a contributing processing center. The default contributing processing center is "ucar", and so the tarball path is prefixed by "http://data.cosmic.ucar.edu/gnss-ro/" implicitly. If the "--romsaf" option is given, then the contributing processing center is the ROM SAF. 

An example AWS Batch command for a liveupdate from the UCAR stream is 
```
["liveupdate_wrapper", "webscrape", "1.1", "--tarfile", "tdx/postProc/level1b/2023/268/conPhs_postProc_2023_268.tar.gz"]
```

Dependencies: None

### Package architecture

![Package architecture](http://github.com/gnss-ro/aws-opendata/blob/master/reformatting_system/docker/images/Architecture.png "Architecture")

The packages in the RORefCat system are structured according to their functionality and for ease of augmentation. It is possible to call any function-method in any of the package modules without having to refer to modules higher in the flowchart. 

Note that the exception/error/warning handling throughout follows a strong convention. 
- Most functions return a dictionary that contains a "status" ("success" or "fail") that indicates whether the function succeeded or failed in execution. The dictionary also contains "messages" and "comments" describing special or noteworthy circumstances in the execution of the function. 
- All output is generated by Python logging. The "warning" level is used to note something wrong in the processing center-provided input files. The "error" level is used to note something wrong with the reformatting code. The logging output can be written to standard output and/or to an output file. In the latter case, the file is copied into an S3 bucket for later diagnostic consultation. 
- All exceptions are raised as defined by Exception child classes and contain the attributes "message" and "comment". Exceptions should be exceptional in the software and demand immediate attention by the maintainers of the software. When running **batchprocess**, even the exception are written into the output logs. 


##### Database

This package provides two classes: a portal to the DynamoDB table that stores metadata and pointers to AWS data files for all RO soundings in the AWS repository; and a wrapper to the reformatting functions in the Reformatters modules. The latter is especially powerful: it interacts with the database to check for the existence of an RO sounding in the repository, creates a new entry in the database for newly contributed RO soudnings, reads metadata from the database or writes newly acquired metadata to the database, and performs the reformatting functions. 

This package should be left static. 

##### Reformatters

This package provides modules that reformats data files contributed by independent RO processing centers into AWS unified/standardized formats as defined in the Versions package. Each module in the package corresponds to one RO processing center. In them, the contents of the RO processing center's RO data files are translated into the AWS variables as defined in Versions. 

This package is extensible by adding a new module for a new contributing processing center. See below. 

##### Versions

This package provides modules that defined output templates for the AWS unified/standardized output formats. Each module defines just one version and contains functions *format_level1b*, etc., that create dimensions, variables, and attributes. The modules do not write data into those variables: that is reserved for the Reformatters package. 

This package is also extensible by adding a new module for a new data format. See below. 

##### Utilities 

This package contains a variety of utilities that are used throughout the Database, Reformatters, and Versions packages. Most important is the module TimeStandards, which provides two classes that can be used to translate between different time standards (UTC, TAI, GPS). Other utilities include the JGM3 gravity model and ECI/ECEF coordinate converters. 

The user is free to add utility modules to this package as long as those already existing are left unaltered. 

##### Missions 

This package contains definitions of the RO missions. Each module contains definition for just one RO mission. It provides information regarding tracking algorithms and satellite naming conventions. 

This package is extensible by adding a new module for a new RO mission. See below. 

##### GNSSsatellites 

This package creates a GNSS satellite history. It provides a function that identifies a unique satellite given a RINEX 3 name (PRN) for the satellite and a time of an RO sounding. 

At present this package accesses the GNSS satellite history information from http://ftp.aiub.unibe.ch/BSWUSER54/CONFIG/SATELLIT_I20.SAT. If this data feed is killed, then the package must be rewritten in order to download and parse another source of the GNSS satellite history data. 

### Extending the system 

It is possible to augment the system to include new contributing processing centers, new RO missions, or newly defined output formats. The system is designed to simplify the process to do so. What follows is a description for how those augmentations should be done. 

#### Add a new processing center

1. You must create a new module in the Reformatters package with a name corresponding to the name of the processing center; e.g., "ucar.py". The new module must contain the following: (a) the string *processing_center* which defines the mnemonic for the processing center as it will be realized in the AWS Registry of Open Data; (b) a dictionay *centerwmo* that defines the WMO identifier for the processing center, containing at least a key "originating_center_id" that points to the integer value of the WMO originating center ID; (c) list that defines the references for the center's processing system (*ionospheric_references*, *optimization_references*, and *retrieval_references*), the elements of each list being strings of the form "doi:..." indicating digital object identifiers; (d) a function *varnames* that returns a dictionary containing the name of the transmitter, receiver, and time of a sounding given a RO data file name; (e) functions that actually perform the reformatting (*level1b2aws*, *level2a2aws*, and/or *level2b2aws*). See any of the existing reformatting modules to use as a template.  
2. Update the **createjobs** executable. **Createjobs** scans a directory hierarchy for RO data files, and the way it does so depends strongly on the directory hierarchy implemented by each processing center. The Python code code of **createjobs** must be updated to know how to scan through the directory hierarchy defined and constructed by the new contributing center. **Createjobs** already is coded to do so for "ucar", "romsaf", "eumetsat", and "jpl"; those sections of the code can be consulted when implementing a new processing center. 
3. Update the satellite naming in the Missions modules to define the satellite naming conventions used by the new processing center. 
4. There is no need to update Reformatters/\_\_init\_\_.py. Leave it untouched. 

#### Add a new RO mission

This can be done by adding a new module to the package Missions. Consult especially the cosmic1.py and planetiq.py modules as templates. 

1. The name of the module should be the same as the mnemonic for the mission in the AWS Registry of Open Data. Include the string variable in the module *mission* to defined the mnemonic for the mission as well. 
2. In the module, define a function *signals* in the module that generates a list of the GNSS signals used in the missions' satellites' tracking algorithm. The arguments to *signals* must be the transmitter name (e.g., "G03"), the AWS receiver name (as described below), and a class datetime.datetime time of the occultation sounding. The function must return a list of dictionaries, each dictionary corresponding to a single GNSS signal. Each dictionary should have the keys "standardName" whose value is a colloquial for the signal name (e.g., "C/A"), the key "rinex3name" whose value is a RINEX 3 observation code for the phase signal (e.g., "L1C"), and a key "loop" that can take the string value "open" for full or partial open-loop tracking or "closed" for strictly closed-loop tracking. 
3. Define the variable *satellites*, which is a list of dictionaries defining each satellite in the mission. The elements of each dictionary the processing centers as keys, each pointing to a dictionary that defines the mission name and satellite name as implemented by the relevant contributing processing center. Two of the processing centers must be "aws", which defines the AWS Registry of Open Data naming conventions for the satellites, and "wmo", which defines the WMO identifiers for the satellite and its RO instrument. 
4. There is no need to update Missions/__init__.py. Leave it untouched. 

#### Add a new format version

This requires creating a new module in the Versions package. 
1. The name of the new module can be anything. 
2. The module attribute/variable *AWSversion* must be a string defining the versions number; e.g., *AWSversion* = "1.1". 
3. A module attribute/variable *file_indexing* must be a dictionary defininig the version's specific naming conventions for the various levels. The keys must be "level1b", "level2a", and "level2b". Each must point to a string that gives the naming convention for those file formats. Consult version1 or version2, for example. 
4. A module attribute/variable *database_variables* must be a dictionary that defines the metadata entires in the DynamoDB table. Each key is the name of the metadata variable. The keys "leo-tnm" and "date-time" are reserved for the partition key and the sort key, respectively. Every other key must point to a Python implicit that defines the data type for the entry in the database; for example, the types can be float, bool, or str. 
5. The module function *defpath* must be defined. It generates a full path to an output file name given the file type ("level1b", "level2a", "level2b"); the name of the processing center; the names of the mission, transmitter, and receiver, all AWS conventions; the time of the occultation as an instance of Utilities.TimeStandards.Calendar; the occultation ID as defined by Database.dynamodbinterface.defoccid; and the version name as defined by the contributing proessing center. The output is a full path with the AWS mission name as the root. 
6. The module functions *format_level1b*, *format_level2a*, and/or *format_level2b*. These functions take occultation metadata as input; create the NetCDF definitions of dimensions, attributes, and variables; and passes back the netCDF4 pointers to those variables in a dictionary. See existing version2 as a template for the required arguments to those functions. Optional arguments that can be used by the reformatting function can be passed in through the extra keyword-argument list *optional*. Pointers to the newly constructed variables in the output file are returned in a dictionary. Standard keys point to the variables. For level1b, those keys are "startTime", "endTime", "navBitsPresent", "snrCode", "phaseCode", "carrierFrequency", "time", "excessPhase", "snr", "rangeModel", "phaseModel", "positionLEO", and "positionGNSS". For level2a, those keys are "refTime", "refLongitude", "refLatitude", "setting", "equatorialRadius", "polarRadius", "undulation", "centerOfCurvature", "impactParameter", "carrierFrequency", "rawBendingAngle", "bendingAngle", "optimizedBendingAngle", "geopotential", "altitude", "longitude", "latitude", "orientation", "refractivity", "dryPressure", "quality", and "superRefractionAltitude". For level2b, those keys are "refTime", "refLongitude", "refLatitude", "geopotential", "altitude", "refractivity", "pressure", "temperature", "waterVaporPressure", "quality", "setting", and "superRefractionAltitude". The units of the variable values to be written into these variable definitions are always SI. 
