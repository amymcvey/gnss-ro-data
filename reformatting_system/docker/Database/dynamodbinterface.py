"""
dynamodbinterface.py
Version 2.0.0
Authors: Stephen Leroy (sleroy@aer.com)
Date: January 27, 2023

This is the module that should be used to interface with the DynamoDB
database used to log occultation files as processed by a variety of
RO processing centers. It provides useful parameters for handling and
organizing the RO data as contributed from the processing centers and
as should be created in the AWS Open Data database. It also provides
two classes for interaction with a DynamoDB database: one that
simplifies interaction with a database for use as one specifically
tailored to logging entries into the AWS Open Data Registry of GNSS-
RO data, and one that wraps RO data reformatting functions in a way
that automatically logs the data being reformatting into a database.

All functions and methods return a dictionary. Mandatory elements of
the returned dictionary are the following keyword-value pairs:

     'status': "success" for successful completion and "fail" for
                unsuccessful completion;
     'messages': a list of mnemonic messages generated by the
                function/method and its child processes; and
     'comments': a list of verbose messages generated by the
                function/method and its child processes.

If the function/method is intended to generate a single scalar value,
that value is pointed to by keyword "value". If the function/method
returns the contents of a DynamoDB occultation entry, the contents
of that entry are pointed to by keyword "Items".

=========================
  class RODataBase
=========================

Initializes with an instance of a DynamoDB Table. As an option,
specify the time window in which an occultation occurs and should
be considered unique, in minutes.

RODataBase.getocc( self, transmitter, receiver, time )
     Get an occultation entry from the database.

RODataBase.createocc( self, transmitter, receiver, time, version )
     Create an occultation entry in the database. It
     does not add information to the entry; it only
     creates a partition key and sort key for the entry.

RODataBase.putoccinfo( self, transmitter, receiver, mission_vars, version,
         mission=None, longitude=None, latitude=None, local_time=None, setting=None,
         processing_center=None, conPhs=None, atmPrf=None, wetPrf=None, bfrPrf=None )
     Add information to an occultation entry in the database.

=========================
  class ProcessReformat
=========================

This class creates a wrapper for low-level routines that reformat
occultation data files that are contributed by independent processing
centers. The wrapper integrates the low-level reformatter with the
DynamoDB data base object, an instance of class RODataBase.

It comes with a wrapper, process_reformat_wrapper, that can be implemented
by a DASK worker.

=========================
  Utility functions
=========================

BEWARE!! Best practice is not to call any of these utility functions
outside of the RODataBase class. The risks are multiple definitions
of any of the outputs they produce. For example, occultation time and
and occid must be defined by the database entry (if it exists for an
occultation) or by the createocc method.

_defoccid( transmitter, receiver, time )
     Define occid for an occultation described by a transmitter,
     a receiver, and a time (class TimeStandards.Calendar).

_defpartitionkey( transmitter, receiver, time )
     Defines the partition key in the Dynamo DB table for an
     occultation defined by a transmitter, a receiver, and a time
     (class TimeStandards.Calendar).

_defsortkey( transmitter, receiver, time )
     Defines the sort key in the Dynamo DB table for an occultation
     defined by a transmitter, a receiver, and a time (class
     TimeStandards.Calendar).

"""


#  Imports.

import re
import os
import shutil
import json
import decimal
import boto3
import numpy as np
from botocore.exceptions import ClientError
from boto3.dynamodb.conditions import Key, Attr
from Utilities.TimeStandards import Calendar, Time
from Reformatters import reformatters, varnames
from Missions import valid_missions, receiver_satellites, get_receiver_satellites
from GNSSsatellites import valid_transmitters

#  Import translator functions for use by process_reformat_wrapper.

from Reformatters import reformatters

#  Logger.

import logging
LOGGER = logging.getLogger(__name__)
DEBUG = False

#  Create a list of valid receivers by concatenating the lists associated with
#  each mission in valid_missions.

valid_receivers = sorted( [ sat['aws']['receiver'] for sat in receiver_satellites ] )

#  Initialize processing reformatters module variable.

global __PROCESSREFORMATTERS__
__PROCESSREFORMATTERS__ = None

#  Fill value for unfilled entries of floats in the database.

fill_float = -999.99


################################################################################
#  Exception handling.
################################################################################

class Error( Exception ):
    pass

class ProcessReformatError( Error ):
    def __init__( self, messages, comments ):
        self.messages = messages
        self.comments = comments


################################################################################
#  Utility functions.
################################################################################

def _defoccid( transmitter, receiver, time ):
    """ Define occultation id given the the transmitter, the receiver, and
    the time of the occultation. The time is an instance of Time.Calendar.
    All names should conform to AWS specifications."""

    LOGGER.debug( "Running _defoccid: " + \
            json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                'time': time.isoformat() } ) )

    #  Initialize.

    ret = { 'status': None, 'messages': [], 'comments': [] }

    #  Check the input.

    if receiver not in valid_receivers:
        ret['status'] = "fail"
        comment = f'Receiver "{receiver}" not recognized as valid'
        ret['messages'].append( "InvalidReceiver" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

    if transmitter not in valid_transmitters:
        ret['status'] = "fail"
        comment = f"transmitter {transmitter} is not recognized as valid"
        ret['messages'].append( "InvalidTransmitter" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

    if not isinstance( time, Calendar ):
        ret['status'] = "fail"
        comment = f"time is not recognized as an instance of Calendar"
        ret['messages'].append( "InvalidTime" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

    #  Occultation ID definition.

    datetimestring = time.datetime().strftime("%Y%m%d%H%M")
    occid = f"{receiver}-{transmitter}-{datetimestring}"

    ret.update( { 'status': "success", 'value': occid } )

    return ret


def _defpartitionkey( transmitter, receiver, time ):
    """Define the parition key for an occultation entry. It will be a function
    of the transmitter name, the receiver name, and the time (instance of class
    Calendar)."""

    LOGGER.debug( "Running _defpartitionkey: " + \
            json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                'time': time.isoformat() } ) )

#  Initialize.

    ret = { 'status': None, 'messages': [], 'comments': [] }

#  Check input.

    if receiver not in valid_receivers:
        ret['status'] = "fail"
        comment = f'Receiver "{receiver}" is invalid'
        ret['messages'].append( "InvalidReceiver" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

    if transmitter not in valid_transmitters:
        ret['status'] = "fail"
        comment = f'Transmitter "{transmitter}" is invalid'
        ret['messages'].append( "InvalidTransmitter" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

#  Create the partition key.

    value = f"{receiver}-{transmitter}"

#  Done.

    ret.update( { 'status': "success", 'value': value } )

    return ret


def _defsortkey( transmitter, receiver, time ):
    """Define the sort key for an occultation entry. It will be a function
    solely of the transmitter name, the receiver name, and the time. The time
    must be an instance of class TimeStandards.Calendar. When considered with
    the partition key, it will uniquely identify a single occultation event."""

    LOGGER.debug( "Running _defsortkey: " + \
            json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                'time': time.isoformat() } ) )

    #  Initialize.

    ret = { 'status': None, 'messages': [], 'comments': [] }

    #  Check input.

    if not isinstance( time, Calendar ): 
        ret['status'] = "fail"
        comment = "time must be an instance of class TimeStandards.Calendar"
        ret['messages'].append( "InvalidArguments" )
        ret['comments'].append( comment )
        LOGGER.error( comment )
        return ret

    #  Create sort key.

    #value = time.isoformat( timespec='seconds' )
    value = f'{time.year:4d}-{time.month:02d}-{time.day:02d}-{time.hour:02d}-{time.minute:02d}'

    #  Done.

    ret.update( { 'value': value } )
    ret['status'] = "success"

    return ret


################################################################################
#  RODataBase class
################################################################################

class RODataBase():

    def __init__( self, session, table_name, timewindow=8 ):
        """Create an instance that accesses a DynamoDB database of radio
        occultation soundings. The arguments are for a boto3 session and
        the name of the DynamoDB table (table_name).

        The timewindow prescribes the time window that uniquely defines an
        occultation, in minutes. Different processing centers define the
        time of an occultation differently, and this time is the maximum
        allowable difference in the time of the same occultation as defined
        by different RO processing centers."""

        dynamoDB = session.resource( 'dynamodb' )
        dynamoDBtable = dynamoDB.Table(table_name)

        try:
            out = dynamoDBtable.creation_date_time
            LOGGER.debug( f"RODataBase.__init__: DynamoDB table {table_name} initialized at {out}" )
        except Exception as excpt:
            LOGGER.error( f"Error reading {table_name}" )
            LOGGER.exception( json.dumps( excpt.args ) )

        self._database = dynamoDBtable
        self._timewindow = timewindow

        return

    def getocc( self, transmitter, receiver, time ):
        """Given a transmitter, receiver, and time of an occultation
        sounding, search the database for the occultation. If it does not
        yet exist in the database, return a status of "fail" and a
        message of "EntryNotFound". The transmitter should be a three-
        character string that defines the transmitter, the first letter
        indicating the GNSS constellation ("G", "R", "E", etc.), and the
        remaining characters being digits the identify the transmitter
        PRN. The receiver is defined according to the AWS manifest and
        returned by the varnames function. The time is an instance of
        class TimeStandards.Calendar."""

        LOGGER.debug( "Running RODataBase.getocc: " + \
                json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                    'time': time.isoformat() } ) )

#  0. Check input.

        if transmitter not in valid_transmitters:
            ret['status'] = "fail"
            comment = f'Transmitter "{transmitter}" is not recognized as valid'
            ret['messages'].append( "InvalidTransmitter" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret

        if receiver not in valid_receivers:
            ret['status'] = "fail"
            comment = f'Receiver "{receiver}" not recognized as valid'
            ret['messages'].append( "InvalidReceiver" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret

        if not isinstance( time, Calendar ):
            ret['status'] = "fail"
            comment = f"Time is not an instance of Calendar"
            ret['messages'].append( "InvalidTime" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret

#  1. Initialization.

        ret = {'Items': None, 'status': None, 'messages': [], 'comments': []}

#  2. Matchup.

        #  Define the partition key.

        response = _defpartitionkey( transmitter, receiver, time )
        if response['status'] == "fail":
            ret['status'] = "fail"
            ret['messages'] = ret['messages'] + response['messages']
            ret['comments'] = ret['comments'] + response['comments']
            return ret
        partitionkey = Key('leo-ttt').eq( response['value'] )

        #  Define the sortkey.

        response = _defsortkey( transmitter, receiver, ( Time( gps=time ) - self._timewindow * 60 ).calendar("gps") )
        if response['status'] == "fail":
            ret['status'] = "fail"
            ret['messages'] = ret['messages'] + response['messages']
            ret['comments'] = ret['comments'] + response['comments']
            return ret
        else:
            sortkey1 = response['value']

        response = _defsortkey( transmitter, receiver, ( Time( gps=time ) + self._timewindow * 60 ).calendar("gps") )
        if response['status'] == "fail":
            ret['status'] = "fail"
            ret['messages'] = ret['messages'] + response['messages']
            ret['comments'] = ret['comments'] + response['comments']
            return ret
        else:
            sortkey2 = response['value']

        sortkey = Key('date-time').between( sortkey1, sortkey2 )

        #  Execute the database inquiry.

        response = self._database.query( KeyConditionExpression = partitionkey & sortkey )

#  3. Check the response for no occultation found and for too many occultations
#     found in the table.

        if response['Count'] != 1:
            ret['status'] = "fail"
            ret['messages'].append( "EntryNotFound" )
            comment = "Occultation entry not found in table"
            ret['comments'].append( comment )
            LOGGER.info( comment )
            return ret

#  4. Successful completion.

        items = response['Items'][0]

        if "time" in items.keys():
            m = re.search( "^(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):([0-9.]+)$", items['time'] )
            if m:
                itemtime = Calendar(
                    year=int(m.group(1)), month=int(m.group(2)), day=int(m.group(3)),
                    hour=int(m.group(4)), minute=int(m.group(5)), second=float(m.group(6)) )
                items['time'] = itemtime
            else:
                ret['status'] = "fail"
                comment = "The database entry for time for this occultation entry is invalid"
                ret['messages'].append( "DatabaseTimeInvalid" )
                ret['comments'].append( comment )
                LOGGER.error( comment )
                return ret

        #  Convert elements of items to Python-native objects.

        for key in [ "orientation", "longitude", "latitude", "local_time" ]:
            if key in items.keys():
                items[key] = float( items[key] )

        for key in [ "setting" ]:

            if key not in items.keys():
                continue
            value = items[key].strip().lower()

            if value in [ "true", "t" ]:
                items[key] = True
            elif value in [ "false", "f" ]:
                items[key] = False
            elif value == "":
                items[key] = None
            else:
                ret['status'] = "fail"
                comment = f'Unable to interpret items[{key}] = "{items[key]}" as a boolean'
                ret['messages'].append( "CannotInterpretBoolean" )
                ret['comments'].append( comment )
                LOGGER.error( comment )
                return ret

        ret['Items'] = items
        ret['status'] = "success"

        return ret


    def createocc( self, transmitter, receiver, time, version ):
        """Create an occultation entry into the database. It will first check if the
        occultation already exists in the database, and, if it does, will return a
        'status' of "success" in the returned dictionary but include a mnemonic of
        "EntryExists" in the returned dictionaries messages. In cases of successful
        completion, the "Items" as returned by getocc are relayed in the returned
        dictionary. If createocc creates a database entry associated with this
        occultation should the occultation not already exist in the database, this
        method generates only the partition key, the sort key, the occultation ID,
        and the time of the occultation for the occultation entry into the database.
        Fields such as longitude, latitude, etc., must be written using the
        putoccinfo method.

        The version argument must be an element of the list Versions.version.

        A dictionary is returned that contains a 'status' ("success" or "fail"),
        mnemonic 'messages', and 'comments', the latter two as lists. (Keywords are
        denoted using single quotes.) If it succeeds, then 'Items' will also be
        returned, containing the information for that occultation contained in
        in the database."""

        LOGGER.debug( "Running RODataBase.createocc: " +
                json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                    'time': time.isoformat() } ) )

        #  Initialize.

        ret = { 'status': None, 'messages': [], 'comments': [] }

        #  Define the database_variables dictionary.

        database_variables = version['module'].database_variables

        #  Check to see if the occultation already exists in the database.

        response_getocc = self.getocc( transmitter, receiver, time )

        ret['messages'] = ret['messages'] + response_getocc['messages']
        ret['comments'] = ret['comments'] + response_getocc['comments']

        if response_getocc['status'] == "success":
            ret['status'] = "success"
            ret['messages'].append( "EntryExists" )
            comment = "Occultation entry already exists in database"
            ret['comments'].append( comment )
            LOGGER.info( comment )
            ret.update( { 'Items': response_getocc['Items'] } )
            return ret

        #  Create a new entry.

        response_defpartitionkey = _defpartitionkey( transmitter, receiver, time )

        ret['messages'] = ret['messages'] + response_defpartitionkey['messages']
        ret['comments'] = ret['comments'] + response_defpartitionkey['comments']

        if response_defpartitionkey['status'] == "fail":
            ret['status'] = "fail"
            comment = "Unable to define partition key"
            ret['messages'].append( "CannotDefinePartitionKey" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret
        else:
            partitionkey = response_defpartitionkey['value']

        response_defsortkey = _defsortkey( transmitter, receiver, time )

        ret['messages'] = ret['messages'] + response_defsortkey['messages']
        ret['comments'] = ret['comments'] + response_defsortkey['comments']

        if response_defsortkey['status'] == "fail":
            ret['status'] = "fail"
            comment = "Unable to define sort key"
            ret['messages'].append( "CannotDefineSortKey" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret
        else:
            sortkey = response_defsortkey['value']

        response_defoccid = _defoccid( transmitter, receiver, time )

        ret['messages'] = ret['messages'] + response_defoccid['messages']
        ret['comments'] = ret['comments'] + response_defoccid['comments']

        if response_defoccid['status'] == "fail":
            ret['status'] = "fail"
            comment = "Unable to define occid"
            ret['messages'].append( "CannotDefineOccid" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret
        else:
            occid = response_defoccid['value']

        LOGGER.debug(f"Creating item with leo-ttt: {partitionkey}, date-time: {sortkey}")

        #  Initialize new Item.

        Item = { 'leo-ttt': partitionkey, 'date-time': sortkey }

        for var, vtype in database_variables.items():
            if vtype is float:
                Item.update( { var: decimal.Decimal( str(fill_float) ) } )
            elif vtype is str:
                Item.update( { var: "" } )
            elif vtype is bool:
                Item.update( { var: "" } )

        #  Mandatory items.

        Item.update( { 'occid': occid } )

        #  Transmitter and receiver as requested in database_variables.

        if "transmitter" in database_variables.keys():
            Item.update( { 'transmitter': transmitter } )
        if "receiver" in database_variables.keys():
            Item.update( { 'receiver': receiver } )
        if "time" in database_variables.keys():
            Item.update( { 'time': time.isoformat() } )

        response_putitem = self._database.put_item( Item=Item )

        #  Get all info on occultation in the table.

        response_getocc = self.getocc( transmitter, receiver, time )

        ret['messages'] = ret['messages'] + response_getocc['messages']
        ret['comments'] = ret['comments'] + response_getocc['comments']

        if response_getocc['status'] == "fail":
            ret['status'] = "fail"
            return ret

        ret.update( { 'status': "success", 'Items': response_getocc['Items'] } )

        return ret


    def putoccinfo( self, transmitter, receiver, time, version,
                   processing_center=None, clobber=False, **metadata ):
        """Assign information values to an occultation entry, which is defined
        according to transmitter, receiver, and time (instance of class
        TimeStandards.Calendar). If the occultation does not yet exist in the
        table, then a 'status' of "fail" is returned in the returned dictionary.
        Information that may have already been entered for an occultation entry
        will be clobbered only if the argument clobber is set to True.

        A dictionary is returned containing 'status' ("success" or "fail"), a list of
        mnemonic 'messages', and a list of verbose 'comments'. An extra field 'Items'
        contains the contents of the DynamoDB occultation entry.

        The argument version must an element of the list Versions.versions.

        Metadata
        ===========
        The metadata to be stored in the database are defined by
        Utilities.config.database_variables.

        Additional variables are the pointers to the data files in AWS. For the
        data files, the keys of metdata should be "{processing_center}_{file_type}",
        where processing_center is the name of the contributing processing center
        and file_type is one of the AWS file types ("level1b", "level2a", etc.).
        """

        LOGGER.debug( "Running RODataBase.putoccinfo: " + \
                json.dumps( { 'transmitter': transmitter, 'receiver': receiver,
                    'time': time.isoformat() } ) )

        #  Initialize.

        ret = { 'status': None, 'messages': [], 'comments': [] }

        #  Check input.

        if processing_center is not None and processing_center not in reformatters.keys():
            ret['status'] = "fail"
            comment = f'Processing center "{processing_center}" not a valid one'
            ret['messages'].append( "InvalidCenter" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            return ret

        if "mission" in metadata.keys():

            mission = metadata['mission']
            if processing_center is None:
                all_missions = []
                for center, center_missions in valid_missions.items():
                    all_missions += center_missions
                if mission not in set( all_missions ):
                    ret['status'] = "fail"
                    comment = f'Mission "{mission}" not a valid one'
                    ret['messages'].append( "InvalidMission" )
                    ret['comments'].append( comment )
                    LOGGER.error( comment )
                    return ret

            else:
                if mission not in valid_missions['aws']:
                    ret['status'] = "fail"
                    comment = f'Mission "{mission}" not a valid one for ' + \
                            f'processing center "{processing_center}"'
                    ret['messages'].append( "InvalidMission" )
                    ret['comments'].append( comment )
                    LOGGER.error( comment )
                    return ret

        response = self.getocc( transmitter, receiver, time )

        #  Occultation entry not created.

        if response['status'] == "fail":
            ret['status'] = "fail"
            ret['messages'] = response['messages']
            ret['comments'] = response['comments'] + [ "Be sure to create the occultation entry" ]
            return ret

        #  List of dicts. Keep only the first element of the list.

        if response['Items'] is not None:
            getocc_items = response['Items']
            ret.update( { 'Items': response['Items'] } )

        #  Interpret information.

        partitionkey = response['Items']['leo-ttt']
        sortkey = response['Items']['date-time']

        #  Check existing info to see if any new information can be added.

        new_info = {}

        for key in list( metadata.keys() ):
            put = False

            #  Check for new metadata.

            if key in version['module'].database_variables.keys():
                vtype = version['module'].database_variables[key]
                if key in getocc_items.keys():
                    if vtype is str:
                        put = ( getocc_items[key] == "" or getocc_items[key] is None or clobber )
                    elif vtype is bool:
                        put = ( getocc_items[key] == "" or getocc_items[key] is None or clobber )
                    elif vtype is float:
                        put = ( float( getocc_items[key] ) == fill_float or clobber )
                else:
                    put = True

            #  Check for new data file pointers.

            else:
                m = re.search( "^([a-z]+)_([a-z0-9]+)$", key )
                if m:
                    vtype = str
                    put = ( m.group(1) in reformatters.keys() )
                    if put:
                        put = put and ( m.group(2) in reformatters[m.group(1)].keys() )
                        put = put and ( clobber or getocc_items[key] == "" )
                else:
                    put = False

            if put:
                if vtype is str:
                    new_info.update( { key: metadata[key] } )
                elif vtype is float:
                    new_info.update( { key: decimal.Decimal( str(metadata[key]) ) } )
                elif vtype is bool:
                    if metadata[key]:
                        new_info.update( { key: "True" } )
                    else:
                        new_info.update( { key: "False" } )

        #  Add paths to data files.

        file_indexing = version['module'].file_indexing

        if processing_center is not None:
            for file_type in reformatters[processing_center].keys():
                if file_type in metadata.keys():
                    new_info.update( { f'{processing_center}_{file_indexing[file_type]}': metadata[file_type] } )

        LOGGER.debug( "RODataBase.putoccinfo: new_info=" + json.dumps( list( new_info.keys() ) ) )

        #  Check if there is new info to add.

        if len( new_info ) == 0:
            LOGGER.debug( f"RODataBase.putoccinfo: No new info to add" )
            ret['status'] = "success"
            ret['messages'].append( "NoInfoAdded" )
            ret['comments'].append( "No new information added" )
            return ret

        #  Establish dynamoDB update params.

        update_expression_str = "set " + ", ".join( [ f"{key} = :{key}" for key in new_info.keys() ] )
        update_values = { f":{key}": val for key, val in new_info.items() }

        #  Update table with values in update_values

        try:
            response = self._database.update_item(
                Key={'leo-ttt': partitionkey, 'date-time': sortkey},
                UpdateExpression=update_expression_str,
                ExpressionAttributeValues=update_values,
                ReturnValues="UPDATED_NEW")
            ret.update( response )
            ret['status'] = "success"

        except ClientError as excpt:
            comment = "Cannot update an item in the DynamoDB table"
            ret['status'] = "fail"
            ret['messages'].append( "ClientErrorException" )
            ret['comments'].append( comment )
            LOGGER.error( comment )
            LOGGER.exception( json.dumps( excpt.args ) )

        #  Done.

        return ret


###############################################################################
#  Define a python wrapper for the ProcessReformat class.
################################################################################

def process_reformat_wrapper( file_type, processing_center,
                             dynamodb_table_name, input_root_path,
                             input_relative_path, output_root_path,
                             clobber=False, workingdir=None, session=None ):
    """This function wraps ProcessReformat. Given an AWS-native RO filetype and the
    processing_center that contributed the input file, return a ProcessReformat
    class callable object. Valid file_type are the AWS-native RO file types. Valid
    processing centers are ucar and romsaf.

    The input arguments are :
    (1) file_type
    (2) processing_center
    (3) dynamodb_table_name
    (4) input_root_path
    (5) input_relative_path
    (6) output_root_path
    (7) (optional) clobber
    (8) (optional) workingdir
    (9) (optional) session

    This function allows the ProcessReformat class to be used inside Dask workers.
    Only provide a session if authentication is necessary in the working compute
    environment."""

    cwd = os.getcwd()
    if DEBUG:
        print(f'   Dask worker CWD = {cwd}')
        LOGGER.debug(f'   Dask worker CWD = {cwd}')

    LOGGER.debug( "Running process_reformat_wrapper: " + \
            json.dumps( { 'file_type': file_type, 'processing_center': processing_center,
                'dynamo_table_name': dynamo_table_name, 'input_root_path': input_root_path,
                'input_relative_path': input_relative_path, 'output_root_path': output_root_path,
                'clobber': clobber, 'workingdir': workingdir, 'profile_name': session.profile_name } ) )

    #  Check input.

    if processing_center not in reformatters.keys():
        comment = f'Processing center "{processing_center}" is not a valid one'
        LOGGER.error( comment )
        return

    if file_type not in reformatters[processing_center].keys() :
        comment = f'File type "{file_type}" is not a valid one for processing center "{processing_center}"'
        LOGGER.error( comment )
        return

    # Add converter utilities and DynamoDB interface software to python
    # system search path so they may be imported.

    #  Process-reformat.

    process_reformatter = ProcessReformat( reformatters[processing_center][file_type],
                file_type, processing_center, dynamodb_table_name,
                workingdir=workingdir, session=session )

    ret = process_reformatter( input_root_path, input_relative_path, output_root_path,
            clobber=clobber )

    #  Done.

    return ret


################################################################################
#  Define a process-reformatter wrapper class.
################################################################################

class ProcessReformat():
    """This class creates a wrapper of a reformatting function that enables
    it to interact with a DynamoDB database. The returned instance is a callable
    function, defined by the magic method __call__, which function will take
    only four arguments. Those arguments are

        (1) the input root path, which defines a local path beneath which
            the data is manifested for reformatting;
        (2) the relative path to the input file to be reformatted beneath
            the input root path; and
        (3) the output root path, which establishes a local root path for the
            output files.

    The input files are generated by joining the input root path (argument 1)
    and the relative path to the file (argument 2). The input root path must
    consist only of locally defined directories. The relative path to the
    input file must be structured exactly the same way as the contributing
    processing center manifests its data, typically with the mission name as
    the highest level directory in the path.

    An optional keyword is clobber (class bool) that prescribes
    whether database entries and output files should be clobbered."""

    def __init__( self, file_type, processing_center, table_name, version,
                 session=None, workingdir=None, purge=True ):
        """Create a wrapper for a function that reformats UCAR, ROM SAF, or
        JPL GNSS RO files to AWS Open Data native formats.

        The file_type (class string) must be one of the accepted AWS defined
        formats.

        The processing_center (class string) must be one of the accepted
        contributing processing centers.

        The table_name is the name of the DynamoDB database table where
        occultations and their metadata are logged.

        The version is an instance of Versions.get_version().

        The session is an element of the list Versions.versions that defines
        the AWS processing/reformatting version.

        The optional argument session is the AWS session to be used to gain
        access to the DynamoDB database. If no session is given, this class
        will attempt to create a session without a security profile and to
        a default region.

        The working directory (workingdir) is used for manifesting S3 files
        either upon ingest or for upload. The default is to have the
        working directory be the home directory + ".local".

        If purge is set to True, all temporary storage in the workingdir
        and the workingdir itself are removed after the reformatting is
        complete.

        The reformatting function that will be wrapped must take eight
        manadatory arguments:

            (1) absolute path to the input file (str);
            (2) absolute path to the output file (str);
            (3) mission name (str);
            (4) transmitter name (str);
            (5) receiver name (str);
            (6) input file type (str);
            (7) processing center version (str), containing no underscores; and
            (8) relative path to the input file (str).

        The mission, transmitter, receiver must conform to valid values."""

        #  Check input.

        if processing_center not in reformatters.keys():
            message = "InvalidProcessingCenter"
            comment = f'Processing center "{processing_center}" is not a valid one'
            LOGGER.error( comment )
            raise ProcessReformatError( message, comment )

        if file_type not in reformatters[processing_center].keys() :
            message = "InvalidFileType"
            comment = f'File type "{file_type}" is not valid for processing ' + \
                    f'center "{processing_center}"'
            LOGGER.error( comment )
            raise ProcessReformatError( message, comment )

        self.reformatting_function = reformatters[processing_center][file_type]
        self.file_type = file_type
        self.processing_center = processing_center
        self.table_name = table_name
        self.purge = purge
        self.version = version

        #  Begin AWS session.

        if session is None:
            self.session = boto3.session.Session()
        else:
            self.session = session

        #  Working directory.

        if workingdir is None:
            self.workingdir = os.path.join( os.path.expanduser( '~' ), ".local" )
        else:
            self.workingdir = workingdir

        return

    def __call__( self, input_root_path, input_relative_path, output_root_path,
            clobber=False ):
        """Reformat an input file contributed by a valid processing center and
        add metadata and paths to the database. The arguments are

            (1) input_root_path, the input root path (class str), which
                establishes a local root path;
            (2) input_relative_path, the relative path to the input file
                (class str) to be reformatted; and
            (3) output_root_path, the output prefix (class str), which
                establishes a local root path for the output files.

        Lastly, an optional keyword is clobber (class bool) that prescribes
        whether database entries and output files should be clobbered.
        Operationally, when arguments (1) and (2) are joined an absolute
        path to the input file to be converted must result."""

        LOGGER.debug( f"Running ProcessReformat: " +
                json.dumps( { 'file_type': self.file_type,
                    'input_root_path': input_root_path,
                    'input_relative_path': input_relative_path,
                    'output_root_path': output_root_path,
                    'clobber': clobber } ) )

        #  Initialize.

        ret = { 'status': None, 'messages': [], 'comments': [] }

        #  Check incoming and outgoing root paths to see if they are S3 buckets.

        s3 = self.session.client( 's3' )

        m = re.search( "^s3://", input_root_path )
        if m:
            path_split = re.split( os.path.sep, input_root_path[5:] )
            bucket = path_split.pop(0)
            if len( path_split ) == 0: path_split = [ "" ]
            local_root = os.path.join( self.workingdir, *path_split )
            input_s3 = {
                    'bucket': bucket,
                    'bucket_root': os.path.join( *path_split ),
                    'local_root': local_root }
        else:
            input_s3 = None

        m = re.search( "^s3://", output_root_path )
        if m:
            path_split = re.split( os.path.sep, output_root_path[5:] )
            bucket = path_split.pop(0)
            if len( path_split ) == 0: path_split = [ "" ]
            local_root = os.path.join( self.workingdir, *path_split )
            output_s3 = {
                    'bucket': bucket,
                    'bucket_root': os.path.join( *path_split ),
                    'local_root': local_root }
        else:
            output_s3 = None

        #  Create RODataBase object.

        database = RODataBase( self.session, self.table_name )

        #  Parse the input file name and create an entry for this occultation in the
        #  database (if one doesn't already exist).

        response_varnames = varnames[self.processing_center]( input_relative_path )

        ret['messages'] = ret['messages'] + response_varnames['messages']
        ret['comments'] = ret['comments'] + response_varnames['comments']

        if response_varnames['status'] == "fail":
            ret['status'] = "fail"
            return ret

        transmitter = response_varnames['transmitter']
        receiver = response_varnames['receiver']
        time = response_varnames['time']
        mission = response_varnames['mission']

        #  Create an entry in the occultation database for this occultation if one
        #  doesn't already exist. The response will contain all information for
        #  the occultation entry, whether it is created or already exists.

        AWSversion = self.version['AWSversion']
        response_createocc = database.createocc( transmitter, receiver, time, self.version )

        #  Add information that's available.

        database.putoccinfo( transmitter, receiver, time, self.version, mission=mission )

        ret['messages'] = ret['messages'] + response_createocc['messages']
        ret['comments'] = ret['comments'] + response_createocc['comments']

        if response_createocc['status'] == "fail":
            ret['status'] == "fail"
            return ret

        #  Load database data for this entry into "extra".

        extra = { key: value for key, value in response_createocc['Items'].items() if
                key not in [ "mission", "transmitter", "receiver" ] }

        #  Compose input path.

        if input_s3 is None:
            input_absolute_path = os.path.join( input_root_path, input_relative_path )
        else:
            input_absolute_path = os.path.join( input_s3['local_root'], input_relative_path )

        #  Compose output path. transmitter and receiver come from varnames;
        #  occid and time come from the database entry.

        response_defpath = self.version['module'].defpath( self.file_type, self.processing_center,
            mission, transmitter, receiver, time,
            response_createocc['Items']['occid'], response_varnames['version'] )

        ret['messages'] = ret['messages'] + response_defpath['messages']
        ret['comments'] = ret['comments'] + response_defpath['comments']

        if response_defpath['status'] == "fail":
            ret['status'] = "fail"
            return ret

        if output_s3 is None:
            output_absolute_path = os.path.join( output_root_path, response_defpath['value'] )
        else:
            output_absolute_path = os.path.join( output_s3['local_root'], response_defpath['value'] )
            output_s3_object = os.path.join( output_s3['bucket_root'], response_defpath['value'] )

        #  Determine whether or not output file should be generated. If clobber is
        #  true, then always reformat. If clobber is False, then generate the
        #  output file only if it doesn't already exist.

        if output_s3 is None:

            if os.path.exists( output_absolute_path ):
                ret['messages'].append( "OutputFileExists" )
                comment = f"Output file {output_absolute_path} already exists"
                ret['comments'].append( comment )
                LOGGER.info( comment )
                reformat = clobber
            else:
                reformat = True

        else:

            reformat = clobber
            try:
                outdir = os.path.split( output_absolute_path )[0]
                os.makedirs( outdir, exist_ok=True )
                s3.download_file( output_s3['bucket'], \
                        output_s3_object, \
                        output_absolute_path )
            except:
                reformat = True

        #  Remove files if no reformatting to take place.

        if not reformat and output_s3 and os.path.isfile( output_absolute_path ):
            os.unlink( output_absolute_path )

        #  Perform the reformat. Harvest metadata from the process of reformatting
        #  and store it in outdict. That information will be written into the
        #  database.

        if reformat:

            ret['messages'].append( "ReformattingInputFile" )
            comment = f"Generating output file {output_absolute_path}"
            ret['comments'].append( comment )
            LOGGER.info( comment )

        #  Fetch input file from S3 if needed.

            if input_s3:
                indir = os.path.split( input_absolute_path )[0]
                os.makedirs( indir, exist_ok=True )
                try:
                    s3.download_file( input_s3['bucket'], \
                        os.path.join( input_s3['bucket_root'], input_relative_path ), \
                        input_absolute_path )
                except Exception as excpt:
                    ret['status'] = "fail"
                    comment = "Cannot download " + "s3://" \
                        + os.path.join( input_s3['bucket'], input_s3['bucket_root'], input_relative_path )
                    ret['messages'].append( "CannotDownload" )
                    ret['comments'].append( comment )
                    LOGGER.error( comment )
                    LOGGER.exception( json.dumps( excpt.args ) )
                    return ret

            #  Create output directory if needed.

            if output_s3 is None:
                path_split = re.split( os.path.sep, output_absolute_path )
                local_directory = os.path.join( *path_split[:-1] )
                os.makedirs( local_directory, exist_ok=True )

            #  Execute reformat.

            try:
                response_reformat = self.reformatting_function(
                    input_absolute_path, output_absolute_path,
                    response_varnames['mission'], response_varnames['transmitter'],
                    response_varnames['receiver'], response_varnames['input_file_type'],
                    response_varnames['version'], input_relative_path, self.version,
                    **extra )

                success_reformat = ( response_reformat['status'] == "success" )
                ret['messages'] = ret['messages'] + response_reformat['messages']
                ret['comments'] = ret['comments'] + response_reformat['comments']
                occmetadata = response_reformat['metadata']

            except Exception as excpt:

                success_reformat = False
                occmetadata = {}

                #  Update output dict.

                ret['status'] = "fail"
                comment = f"Cannot reformat {output_absolute_path}"

                ret['messages'].append( "CannotReformat" )
                ret['comments'].append( comment )

                #  Log the results.

                LOGGER.error( comment )
                LOGGER.exception( json.dumps( excpt.args ) )

            #  Remove file if it has been (partially/incorrectly) created.

            if not success_reformat:
                ret['status'] = "fail"
                if os.path.exists( output_absolute_path ):
                    os.unlink( output_absolute_path )

            #  Upload output file to S3 if needed.

            if success_reformat and output_s3 is not None :

                try:
                    s3.upload_file( output_absolute_path, output_s3['bucket'], output_s3_object )
                    comment = "Uploaded to " + os.path.join( "s3://", output_s3['bucket'], output_s3_object )
                    ret['comments'].append( comment )
                    LOGGER.info( comment )

                except Exception as excpt:
                    ret['status'] = "fail"
                    comment = "Cannot upload " + os.path.join( "s3://", output_s3['bucket'], output_s3_object )

                    ret['messages'].append( "CannotUpload" )
                    ret['comments'].append( comment )

                    LOGGER.error( comment )
                    LOGGER.exception( json.dumps( excpt.args ) )

        else:

            comment = f"No attempt to generate {output_absolute_path}"
            ret['comments'].append( comment )
            LOGGER.info( comment )

            #  Post-processing: failed reformatting and initializing outdict.

            occmetadata = {}
            success_reformat = False

        #  Complete the population of outdict.

        occmetadata.update( { 'mission': response_varnames['mission'] } )
        if success_reformat:
            occmetadata.update( { 'processing_center': self.processing_center,
                    self.file_type: response_defpath['value'] } )

        #  Log all new information in the database.

        comment = f"Logging the occultation in database"
        ret['messages'].append( "LoggingEntryInDatabase" )
        ret['comments'].append( comment )
        LOGGER.info( comment )

        #  Remove time, clobber from occmetadata.

        for v in [ 'time', 'clobber' ]:
            occmetadata.pop( v, None )

        #  Put database info in table.

        response_putoccinfo = database.putoccinfo( transmitter, receiver, time,
                self.version, clobber=clobber, **occmetadata )

        ret['messages'] = ret['messages'] + response_putoccinfo['messages']
        ret['comments'] = ret['comments'] + response_putoccinfo['comments']
        success_putoccinfo = ( response_putoccinfo['status'] == "success" )

        #  Remove local files.

        if input_s3 is not None and os.path.exists( input_absolute_path ):
            os.unlink( input_absolute_path )

        if output_s3 is not None and os.path.exists( output_absolute_path ):
            os.unlink( output_absolute_path )

        #  Evaluate exit status.

        if success_reformat and success_putoccinfo:
            ret['status'] = "success"
        else:
            ret['status'] = "fail"

        #  Retain the output file name.

        ret.update( { 'output_file': response_defpath['value'] } )

        #  Return the entire occultation entry.

        ret.update(
                { key: value for key, value in
                    database.getocc( transmitter, receiver, time ).items()
                    if key not in [ 'status', 'messages', 'comments' ] } )

        #  Purge the working directory.

        if self.purge and os.path.isdir( self.workingdir ):
            LOGGER.info( f"Removing the tree {self.workingdir}" )
            shutil.rmtree( self.workingdir )

        return ret
